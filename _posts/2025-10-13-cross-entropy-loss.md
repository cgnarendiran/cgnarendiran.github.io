---
layout: post
title:  "Cross Entropy Loss"
date:   2025-10-13
image:  images/blog24/cover.jpg
tags:  Cross Entropy Loss Information Theory KL Divergence
---
*On the cover: Cross Entropy Loss*

Cross Entropy Loss is a loss function that is used to measure the difference between the predicted probability distribution and the true probability distribution. It is a measure of the uncertainty of the model's predictions.

That just sounds like a bunch of jargon doesn't it. Well, let's break it down. All of this is rooted in Information Theory. Claude Shannon-who essentially invented the entire field of information theory-asked a deceptively simple question:

> **â€œHow do we measure information in a way that makes sense?â€**

He proposed three axioms for an information measure (H(p)):

### **Shannonâ€™s Axioms**

1. **Continuity**
   (H(p)) should change smoothly as probabilities change.

2. **Maximization**
   For a fixed number of outcomes, entropy is maximized when all outcomes are equally likely.
   (A perfectly fair die is the most chaotic die.)

3. **Additivity (a.k.a. the â€œChain Ruleâ€)**
   If a decision can be broken into independent parts, total entropy is the weighted sum of entropies.
   [
   H(X, Y) = H(X) + H(Y|X)
   ]

Shannon proved that the **only** function satisfying these axioms (up to a constant factor) is:

[
H(p_1,\dots,p_n) = -K \sum_i p_i \log p_i
]

Set (K = 1) and we get the familiar form.

---

# ğŸ”¥ 2. Self-Information: Surprise in a Single Number

Shannon also defined the information content of a single event:

[
I(x) = -\log p(x)
]

Properties:

* When (p(x)) is small â†’ big surprise
* When (p(x)) is large â†’ small surprise
* When (p(x) = 1) â†’ (I(x) = 0) (no surprise)

Example:
Your friend says â€œgravity still works today.â€
Probability = 1.
Surprise = 0 bits.

Your friend says â€œthe stock market went up because Mercury is in retrograde.â€
Surprise = âˆ bits.

---

# ğŸ”¥ 3. Entropy: Expected Surprise

Entropy measures the *average* amount of self-information:

[
H(p) = \mathbb{E}_{x \sim p}[ -\log p(x) ]
]
[
H(p) = -\sum_x p(x) \log p(x)
]

Alternative continuous form:

[
H(p) = -\int p(x) \log p(x), dx
]

Entropy reflects how uncertain the world is.

* High entropy â†’ chaotic, unpredictable
* Low entropy â†’ predictable, boring (like a model that always predicts â€œcatâ€)

---

# ğŸ¥Š 4. Cross Entropy: When Your Beliefs Are Wrong

Cross entropy is:

[
H(p, q) = -\sum_x p(x) \log q(x)
]

Compare with entropy:

[
H(p) = -\sum_x p(x) \log p(x)
]

Interpretation:

> **â€œHow many bits do I need to encode messages drawn from (p) if I assume the world is (q)?â€**

This is exactly what we do in machine learning:

* (p) = true labels
* (q) = modelâ€™s predicted distribution

For one-hot labels, if true class is (y):

[
H(p,q) = - \log q(y)
]

This is the famous per-sample classification loss.

---

# ğŸ’£ 5. Negative Log-Likelihood (NLL): The ML Version

Likelihood for a sample (x):

[
\mathcal{L} = q(x)
]

Negative log-likelihood loss:

[
\text{NLL} = -\log q(x)
]

Averaged over the dataset:

[
\frac{1}{N} \sum_{i=1}^N -\log q(x_i)
]

This is **exactly** the empirical cross entropy:

[
H_{\text{empirical}}(p, q)
]

Why log?

* Turns products into sums
* Penalizes confident wrong predictions heavily
* Leads to convex/log-concave objectives (nice gradients)
* Sine/cosine would oscillate and destroy the loss landscape (and your model)

---

# ğŸ§© 6. KL Divergence: Distance Between Belief Systems

Defined as:

[
D_{\text{KL}}(p,||,q) = \sum_x p(x) \log \frac{p(x)}{q(x)}
]

Equivalent form:

[
D_{\text{KL}}(p,||,q) = H(p, q) - H(p)
]

Interpretation:

> **Extra surprise caused by using the wrong model (q)**

Properties:

* (D_{\text{KL}} \ge 0)
* (D_{\text{KL}}(p||q) = 0) only when (p = q)
* Not symmetric:
  [
  D_{\text{KL}}(p||q) \neq D_{\text{KL}}(q||p)
  ]

KL divergence is everywhere:

* Variational inference
* Regularization in VAEs
* Policy updates in RL (Trust Region methods)
* Comparing distributions
* Aligning model predictions with truth

---

# ğŸ¯ 7. Importance Sampling: When Rare Events Refuse to Show Up

Suppose you want to estimate:

[
\mu = \mathbb{E}_{x \sim p}[f(x)]
]

But sampling from (p) rarely hits the events that matter (e.g., finance tail losses).

Rewrite expectation using another distribution (q):

[
\mu = \int f(x) p(x) , dx
]

Multiply & divide by (q(x)):

[
\mu = \int f(x) \frac{p(x)}{q(x)} q(x), dx
]

Now sample from (q):

[
\hat{\mu} = \frac{1}{N} \sum_{i=1}^N f(x_i), w(x_i)
]

where
[
w(x) = \frac{p(x)}{q(x)}
]
is the **importance weight**.

Interpretation:

> You sample from a convenient distribution (q)
> and then â€œfixâ€ the bias using the ratio (p/q).

### Example: Rare Events in Finance

Want the probability of a 5-sigma loss:

* Sampling from the real distribution wonâ€™t give you enough 5-sigma events
* You choose a biased distribution that produces more extreme returns
* Apply importance weights to correct the bias
* Estimator now converges with far fewer samples

Importance sampling is basically:

> **â€œCheat smartly, and then correct the cheat mathematically.â€**

---

# ğŸ”— 8. Deep Connections

All these concepts are unified:

* **Self-information**: surprise of one event
* **Entropy**: expected surprise under true distribution
* **Cross entropy**: expected surprise under model distribution
* **NLL**: empirical cross entropy
* **KL divergence**: the price you pay for being wrong
* **Importance sampling**: fixing the price by reweighting surprise

The common thread?
They all revolve around:

[
-\log(\text{probability})
]

This quantity is the currency of information.

